---
layout: default
title: bsub settings and run/CPU/waiting time on Minerva
tags: [reproducible-research, computation, LSF]
featimg: "aligners-realtime-size-1.png"
---

TODO



## Abstract

The following analysis examines how parallelized programs `pigz`, `bwa`, `bowtie2` use Minerva's resources while running on various fastq files and various cores distributed in various ways among the computational hosts.

The following resources are considered as response variables:

1. real time (wall time) of run
1. CPU time of run
1. maximum memory
1. waiting time in queue

The following variables and resources are considered as explanatory variables:

1. machine: manda or mothra
1. number of cores (i.e. processors) specified by the `-n` option of LSF's `bsub`
1. number of cores/host specified by the `ptile` parameter of the `span` string (see LSF's resource requirements)
1. size of the input fastq file (actually it's the sum for two files: forward and reverse reads)
1. requested real (wall) time
1. queue


## Data and computations

### Scripts

The runs and the parsing of `bsub` reports were performed by various high and lower-level scripts: `doruntimes.sh`, `parse-n-sync.sh`, `parser-main.sh`, `parser-sub.sh` and `parse-stdout.sed`.  The parsed results were formatted as CSV:


```r
aligners <-
    list(nproc8 = read.csv("../../results/bsub-profiles/aligners/machine-nproc-ptile.csv"),
         nproc32 = read.csv("../../results/bsub-profiles/aligners/nproc32.csv"),
         threadopt = read.csv("../../results/bsub-profiles/aligners/aligner-threads-opt.csv"))
# new ordering of levels: small medium large huge
aligners$nproc8$size <- factor(aligners$nproc8$size, levels = rev(levels(aligners$nproc8$size)))
```

### Input fastq files

Four pairs of fastq files were chosen based on their size:


```
##      size    fastq.bytes fastq.gz.bytes  read.count     base.count
## 57  small     93,499,712     20,599,818     455,196     34,594,896
## 59 medium  1,027,109,250    225,213,233   4,975,918    373,193,850
## 61  large  7,689,748,378  1,992,327,138  21,443,290  3,237,936,790
## 63   huge 67,032,371,398  1,992,327,138 186,924,226 28,225,558,126
```

These numbers are derived from the **sum** of the forward and reverse fastq file for each pair. Interestingly, the files had various read lengths:


```
##      size read.length
## 57  small          76
## 59 medium          75
## 61  large         151
## 63   huge         151
```

Consequently the uncompressed file size depends non-linearly on read count (which doesn't incorporate read length) but linearly on base count (which does) as the next plots show; in these plots the read length is indicated above the blue 'o' symbols.



<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/basecount-readcount-size-1.png" title="plot of chunk basecount-readcount-size" alt="plot of chunk basecount-readcount-size" width="700px" />

### Requested resources: hosts, cores, wall time

The following `nproc`--`ptile` configurations were run for each aligner, machine, and data size (except for the huge data):


```
##       A B C D E F G H I J
## nproc 1 2 2 4 4 4 8 8 8 8
## ptile 1 1 2 1 2 4 1 2 4 8
```

Real (wall) time (see `-W` option for `bsub`) was requested based on the size and the `ptile` and `machine` and independently from `nproc` and `aligner`:


```
## $manda
##        ptile=1 ptile=2 ptile=4 ptile=8
## small       16       8       4       2
## medium      80      40      20      10
## large      800     400     200     100
## huge      8000    4000    2000    1000
## 
## $mothra
##        ptile=1 ptile=2 ptile=4 ptile=8
## small        8       4       2       1
## medium      40      20      10       5
## large      400     200     100      50
## huge      4000    2000    1000     500
```

Only for the small dataset and manda a larger set of `nproc`--`ptile` configuration was also used, which involved up to 32 processors (cores):


```
##       A B C D E F G H I J  K  L  M  N  O  P  Q  R  S  T  U
## nproc 1 2 2 4 4 4 8 8 8 8 16 16 16 16 16 32 32 32 32 32 32
## ptile 1 1 2 1 2 4 1 2 4 8  1  2  4  8 16  1  2  4  8 16 32
```

For these computations real times were requested accordingly.

## Results: aligners

Before turning to the results of real interest here are some concerning multithreading options.

### Aligners' multithreading option

Aligners are parallelized but must be told the number of threads (e.g. with the `-t` option of `bwa`).  Should `ptile` or `nproc` be given as the number of threads?  To answer that runs were done both ways for `bwa`, `bowtie2` and the small fastqu file for the following configurations:


```
##       A B C D E F G H I J K L M N
## nproc 4 4 4 8 8 8 8 4 4 4 8 8 8 8
## ptile 1 2 4 1 2 4 8 1 2 4 1 2 4 8
```



The next plot shows that it doesn't matter if `ptile` or `nproc` is given as the number of threads to aligners.


```
##       A B C D
## nproc 1 2 4 8
## ptile 1 2 4 8
```

Thus for all subsequent analysis `nproc` was used.

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/aligner-threadopt-realtime-1.png" title="plot of chunk aligner-threadopt-realtime" alt="plot of chunk aligner-threadopt-realtime" width="500px" />

### Real time

#### Dependence on size

The following plot shows that...

* for a given `nproc` (and `ptile` which equals `nproc` in these cases) real time depends linearly on base count (and hence uncompressed file size);  this linearity will be exploited below for fitting linear models
* `bowtie2` and `bwa` are similarly fast and run twice as fast on mothra than on manda
* multithreading is efficient: $$8$$ cores means roughly $$8\times$$ speed

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/aligners-realtime-size-1.png" title="plot of chunk aligners-realtime-size" alt="plot of chunk aligners-realtime-size" width="700px" />

#### Dependence on core-host topology

The following plots show real times under the wider set of conditions already mentioned, namely:


```
##       A B C D E F G H I J
## nproc 1 2 2 4 4 4 8 8 8 8
## ptile 1 1 2 1 2 4 1 2 4 8
```

The main conclusion from these plots is clear: distributing processors onto multiple hosts slows down computation.  In other words, there is no advantage of requesting `ptile` more cores on another host when there is already `ptile` number of cores on a single host.


```r
dotplot.response(df = aligners$nproc8, response = "real.time", m = "mothra")
```

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/aligners-realtime-topology-mothra-1.png" title="plot of chunk aligners-realtime-topology-mothra" alt="plot of chunk aligners-realtime-topology-mothra" width="700px" />

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/aligners-realtime-topology-manda-1.png" title="plot of chunk aligners-realtime-topology-manda" alt="plot of chunk aligners-realtime-topology-manda" width="700px" />

The following plot shows, in addition to the previous ones, that `bwa` can take advantage of 16 and 32 threads more efficiently than `bowtie2` relative to fewer threads:

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/aligners-nproc32-realtime-topology-manda-1.png" title="plot of chunk aligners-nproc32-realtime-topology-manda" alt="plot of chunk aligners-nproc32-realtime-topology-manda" width="700px" />

#### Linear model: real time $$\sim$$ basecount

These models are fitted onto log-transformed data.  Base 60 is used for real time and base 10 for the base count.


```r
M <- lapply(list(manda = "manda", mothra = "mothra", bode = "bode"),
            function(m)
                lapply(list(bowtie2 = "bowtie2", bwa = "bwa"),
                               function(a)
                                   lm(log(real.time, base = 60) ~ log10(base.count), data = aligners$nproc8, subset = ptile == nproc & nproc == 8 & aligner == a & machine == m)))
df <- data.frame(t(sapply(unlist(M, recursive = FALSE), coef)))
names(df) <- c("beta0", "beta1")
df$y <- as.character(formula(M[[1]][[1]]))[2]
df$x <- as.character(formula(M[[1]][[1]]))[3]
write.csv(df, "../../results/bsub-profiles/aligners/realtime-basecount-lm.csv")
df[1:2]
```

```
##                    beta0     beta1
## manda.bowtie2  -3.238289 0.5435233
## manda.bwa      -3.174539 0.5363554
## mothra.bowtie2 -3.053635 0.5014354
## mothra.bwa     -3.254791 0.5240123
## bode.bowtie2   -2.396997 0.4423588
## bode.bwa       -2.314946 0.4330841
```

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/realtime-basecount-lm-1.png" title="plot of chunk realtime-basecount-lm" alt="plot of chunk realtime-basecount-lm" width="700px" />

### CPU time

Like real time CPU time is also linear in fastq size (expressed as basecount or as bytes for the uncompressed state), and the other observations for real time also hold for CPU time *except* that CPU time does not depend on the total number of cores.  The next plot shows this for the more restricted condition


```
##       A B C D
## nproc 1 2 4 8
## ptile 1 2 4 8
```

As the plot shows, the total CPU time is roughly same for all number of processors confirming that multithreading is efficient:

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/aligners-cputime-size-1.png" title="plot of chunk aligners-cputime-size" alt="plot of chunk aligners-cputime-size" width="700px" />

Turning to the wider set of conditions...


```
##       A B C D E F G H I J
## nproc 1 2 2 4 4 4 8 8 8 8
## ptile 1 1 2 1 2 4 1 2 4 8
```

...we see that the total CPU time is independent also from the processor--host topology (`nproc`--`ptile` relationship):

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/aligners-cputime-topology-mothra-1.png" title="plot of chunk aligners-cputime-topology-mothra" alt="plot of chunk aligners-cputime-topology-mothra" width="700px" />

### Maximum memory

Memory usage shows little (`bwa`) or no (`bowtie2`) dependence on conditions suggesting that it is the index file (the reference genome) that consumes the most memory.  For all runs 4 GB memory / core was requested with `bsub -M 4096`.

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/aligners-maxmem-mothra-1.png" title="plot of chunk aligners-maxmem-mothra" alt="plot of chunk aligners-maxmem-mothra" width="700px" />

### Waiting time in queue

How long a job is pending between submission and dispatch depends on several things:

1. the required resources of the job
1. the selected queue
1. the user's priority based on his/her previous activity and the configuration of the fairshare scheduling policy
1. other jobs and their reserved slots (backfill scheduling)
1. the priority of the queue

Among these, the user has exclusive control on the first two point but only some or no control on the other points.  Therefore the following plot needs to be interpreted carefully.  What is clear, though, is the effect of requested wall time and of the selected queue: in the premium queue even the huge jobs (500 - 2000 minute requested wall time) are dispatched before some of the somewhat smaller jobs waiting in the alloc queue.  Note that each plotting symbol is `nproc`, the number of total cores, while its color is coded according to `ptile`.

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/aligners-timeinqueue-1.png" title="plot of chunk aligners-timeinqueue" alt="plot of chunk aligners-timeinqueue" width="700px" />

## Parallel gzip: pigz

Fastq files are normally stored and analyzed in gzip-compressed form. `pigz` is a parallelized variant of `gzip`.  The following analysis examines how `pigz` uses Minerva's resources while running on fastq files and 16 cores distributed in various ways among the computational hosts.  The results broadly agree with those for the aligners.  One peculiarity is that on mothra multithreading with 4 cores is less efficient than what might be expected based on 2 and 8 cores.


```r
pigz <- read.csv("../../results/bsub-profiles/pigz/runtimes-pigz-NA.csv")
```

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/pigz-real-time-1.png" title="plot of chunk pigz-real-time" alt="plot of chunk pigz-real-time" width="700px" />

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/pigz-cpu-time-1.png" title="plot of chunk pigz-cpu-time" alt="plot of chunk pigz-cpu-time" width="700px" />

<img src="{{ site.baseurl }}/projects/tests/R/2017-04-04-runtimes/figure/pigz-max-mem-1.png" title="plot of chunk pigz-max-mem" alt="plot of chunk pigz-max-mem" width="700px" />

<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
